**앙상블 학습**

여러 개의 분류기를 생성하고, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법을 말한다.

강력한 하나의 모델을 사용하는 대신에 보다 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움을 주는 방식이다. 즉 집단 지성이라고 생각하면 편하다.

전 세계의 머신러닝 개발자들의 기량을 겨루는 캐글에서 XGBoost, LightGBM과 같은 앙상블 알고리즘이 인기가 있다.

앙상블 학습은 일반적으로 Voting, Bagging, Boosting 세 가지 유형으로 나눌 수 있다.

Voting

여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식

서로 다른 알고리즘을 여러 개 결합하여 사용

하드 보팅 : 다수의 분류기가 예측한 결과값을 최종 결과로 선정

소프트 보팅 : 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 값을 최종 결과로 선정



Bagging -> random forest 알고리즘이 대표적으로 속한다.

데이터 샘플링을 통해 모델을 학습시키고 결과를 집계하는 방법

모두 같은 유형의 알고리즘 기반의 분류기를 사용

데이터 분할 시 중복을 허용

Boosting 

여러 개의 분류기가 순차적으로 학습을 수행

이전 분류기가 예측을 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게 가중치를 부여하면서 학습과 예측을 진행 

계속하여 분류기에게 가중치를 부스팅하며 학습을 진행하기에 부스팅 방식이라고 불림.



앙상블 학습법에는 두 가지가 있다. 배깅과 부스팅 입니다.  이를 이해하기 위해서는 부트스랩과 결정 트리에 대한 개념이 선행되어야 한다. 



**부트스트랩** 

모수의 분포를 추정하는 파워풀한 방법은 현재 있는 표본에서 추가적으로 표본을 복원 추출하고 각 표본에 대한 통계량을 다시 계산하는 것이다. 이러한 절차를 부트스태랩이라고 한다. 부트스트랩은 데이터가 정규분포를 따라야 한다는 가정이 꼭 필요하지 않습니다. 1억 개의 모집단에서 뽑은 200개의 표본이 있다고 하자. 200개로만 통계량을 구하는 것이 아니라 200개를 기준으로 복원 추출하여 새로운 통계량을 구하는 것이다.

부트스트랩으로 신뢰구간을 구하는 절차이다.

1. 200개의 표본 중 하나를 뽑아 기록하고 다시 제자리에 둔다.
2. 이를 N번 반복한다.
3. n번 재표본추출한 값의 평균을 구한다.
4. 이 단계를 부트스트랩 반복 횟수만큼 반복을 한다.
5. 평균에 대한 결과 R개를 사용하여 신뢰구간을 구한다.

이 방법을 사용하면 표본이 200개 밖에 없을지라도 부트스태랩을 통해 200개 보다 더 많은 통계량을 구할 수 있다. 따라서 부트스트랩을 활용하면 모수를 더 정확하게 추정할 수 있다. 



**결정 트리**

결정 트리는 분류와 회귀 모두 가능한 지도 학습 모델 중 하나이다. 결정 트리는 스무고개 하듯이 예/아니오 질문을 이어가며 학습한다. 특정 기준에 따라 데이터를 구분하는 모델을 결정 트리 모델이라고 한다. 결정 트리 알고리즘의 프로세스를 간단히 알아보면, 데이터를 가장 잘 구분할 수있는 질문을 기준으로 나눈다. 나뉜 각 범주에서 또 다시 데이터를 가장 잘 구분할 수 있는 질문을 기준으로 나눈다. 이를 지나치게 많이 하면 아래와 같이 오버 피팅이 된다. 결정 트리에 아무 파라미터를 주지 않고 모델링하면 오버 피팅이 된다.

이러한 오버피팅을 막기 위한 전략으로 "가지치기(Pruning)"라는 기법이 있다.  최대 깊이나 터미널 노드의 최대 개수, 혹은 한 노드가 분할하기 위한 최소 데이터 수를 제한하는 것이다. min_sample_split 파라미터를 조정하여 한 노드에 들어 있는 최소 데이터 수를 정해줄 수 있다. -> min_sample_split=10이면 한 노드에 10개의 데이터가 있다면 그 노드는 더 이상 분기를 하지 않는 느낌.. 또한 max_depth를 통해서 최대 깊이를 지정해 줄 수도 있다. 

**알고리즘 : 엔트로피(Entropy), 불순도(Impurity) **

불순도란 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻한다. 엔트로피는 불순도를 수치적으로 나타낸 척도이다. 엔트로피가  높다는 것은 불순도가 높다는 뜻이고, 엔트로피가 낮다는 것은 불순도가 낮다는 뜻이다. 엔트로피가 1이면 불순도가 최대라고 말할 수 있다.

정보획득 : 엔트로피가 1인 상태에서 0.7인 상태로 바뀌었다면 정보 획득은 0.3 입니다. 분기 이전으 ㅣ엔트로피에서 분기 이후의 엔트로피를 뺀 수치를 정보 획득이라고 합니다.

정보 획득 : 분기 이전 엔트로피 - 분기이후 엔트로피 * 가중 평균

범주가 하나라면 위 엔트로피 공식으로 바로 엔트로피를 구할 수 있다. 하지만 범주가 2개 이상일 경우 가중 평균을 활용하여 분기 이후 엔트로피를 구한다. **결정 트리 알고리즘은 정보 획득을 최대화하는 방향으로 학습이 진행**된다. 어느 feature의 어느 분기점에서 정보 획득이 최대화 되는지 판단을 해서 분기가 진행된다.

**배깅(Bagging)**

Bagging은 Booststrap Aggresgation의 약자입니다. 배깅은 샘플을 여러 번 뽑아(Booststrap) 각 모델을 학습시켜 결과물을 집계하는 방법입니다.

데이터로부터 부트 스트랩을 한 뒤 그 데이터로 모델을 학습시킨다. 그리고 학습된 모델의 결과를 집계하여 최종 결과 값을 구한다. Categorical Data는 투표 방식으로 결과를 집계, Continuous Data는 평균으로 집계를 한다.

Categorical DAata일 때, 투표 방식으로 한다는 것은 전체 모델에서 예측한 값 중 가장 많은 값을 최종 예측값으로 선정한다는 것이다. 6개의 결정 트리 모델이 있다고 할 때, 4개는 A로 예측했고, 2개는 B로 예측했다면 투표에 의해 4개의 모델이 선택한 A를 최종 결과로 예측한다.

평균으로 집계한다는 것은 말 그대로 각각의 결정 트리 모델이 예측한 값에 평균을 취해 최종 Bagging model의 예측값을 결정한다는 것이다.

**부스팅**

부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법입니다. 배깅은 결정 트리1과 결정 트리2가 서로 독립적으로 결과를 예측합니다. 여러 개의 독립적인 결정 트리가 각각 값을 예측한 뒤, 그 결과 값을 집계해 최종 결과 값을 예측하는 방식입니다. 하지만 부스팅은 모델 간 팀워크가 이루어집니다. 처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 줍니다. 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복한다.

잘못 분류가 된 데이터는 가중치를 높여주고, 잘 분류된 데이터는 가중치를 낮추어 준다. 분류가 잘못된 데이터에 가중치를 부여해주는 이유는 다음 모델에서 더 집중해 분류하기 위함입니다. 

배깅은 병렬로 학습하는 반면, 부스팅은 순차적으로 학습합니다. 한 번 학습이 끝난 후 결과에 따라 가중치를 부여한다. 그렇게 부여된 가중치가 다음 모델의 결과 예측에 영향을 준다. 오답에 대해서는 높은 가중치를 부여하고, 정답에 대해서는 낮은 가중치를 부여한다. 따라서 오답을 정답으로 맞추기 위해 오답에 더 집중할 수있게 되는 것이다.

부스팅은 배깅에 비해 에러가 적다. 즉 성능이 좋지만, 속도가 느리고 오버 피팅이 될 가능성이 있다. 즉 상황에 맞추어 적절하게 사용하는 것이 중요하다.

**데이터 분석**

데이터 분석가들은 필터를 사용해서 이미지 차원을 효과적으로 축소할 수 있지 않을까 생각을 했다. 그리고 필터를 적용한 결과 얕은 기계 학습에 사용했다. 하지만 결과가 좋지 않았다. 이미지 정보를 요약하면서 얻게 되는 유용한 정보의 양보다 과정 속에서 손실되는 정보가 많았기 때문이다.

딥러닝 기계학습을 베이스라인으로 두는 것 보다는 피처 엔지니어링에 비중을 더 많이 둔다.

XGBoost

욕심쟁이를 사용하여 분류기 M, G, H를 발견하고, 분산처리를 사용하여 빠른 속도로 적합한 비중 파라미터를 찾는 알고리즘이다. 분류기는 Regression score을 사용하여 정확도 스코어를 측정하고 각 순서에 따라 강한 분류기부터 약한 분류기까지 랜덤하게 생성된다. 이렇게 만들어진 분류기를 트리(tree)라고 하며, 분류기를 조합한 최종 알고리즘을 포레스트라고 한다. <- 기본적인 boosting algorithm

XGBoost는 트리를 만들 떄 classification and regression trees라 불리는 앙상블 모델을 사용한다. 이후 트리 부스팅을 사용하여, 각 분류 기간 비중을 최적화 한다. cart 모델은 일반적인 의사결정 트리와 조금 다르다. 리프 노드 하나에 대해서만 디시전 벨류를 갖는 의사 결정 트리와는 달리 cart 방식은 몯느 리프들이 모델의 최종 스코어에 연관되어 있다. 따라서 의사결정 트리가 분류를 제대로 했는지에 대해서만 초점을 맞추는 반면, cart는 같은 분류 결과를 갖는 모델끼리도 모델의 우위를 비교할 수 있다. 즉 모든 트레이닝 세트 X에 대하여 포레스트에 넣고, 결괏값으로 나오는 점수의 절댓값을 더한다. 많은 데이터를 + - 방향으로 보낼수록, 좋은 분류 모델이라 할 수 있다.

부스팅이란 라운드가 지날 수 록 모델의 에러를 줄이는 과정이다. 

 xgboost의 파라미터에는 크게 3가지 종류가 있다.

- 일반 파라미터 : 도구의 모양을 결정함
  - booster : 어떤 부스터 구조를 쓸 지 결정 (gbtree, gblinear, dart)
  - nthread : 몇 개의 쓰레드를 동시에 처리하도록 할 지 결정 (디폴트는 최대)
  - num_feature : feature 차원의 숫자를 정해야 하는 경우 옵션을 세팅 (디폴트는 최대)
- 부스터 파라미터 : 트리마다 가지를  칠 때 적용하는 옵션을 정의
  - eta : learning rate, 트리에 가지가 많을 수록 오버피팅 하기 쉽다. 매 부스팅 스탭마다 weight를 주어 부스팅 과정에 오버피팅이 일어나지 않도록 한다.
  - gamma : 정보 획득에서 -r로 표현한 바 있다. 이것이 커지면 트리 깊이가 줄어들어 보수적인 모델이 된다. 디폴트는 0
  - max_depth : 한 트리의 최대 깊이. 숫자를 키울수록 모델의 복잡도가 커진다. 오버피팅 하기 쉬워진다. 디폴트는 6, 리프 노드의 최대 개수 : 2^6
  - lambda : L2 정규화 형태에 달리는 weight이다 숫자가 클수록 보수적인 모델이 된다.
  - alpha : L1 정규화 형태 가중치이다. 숫자가 클수록 보수적인 모델이 된다.
- 학습 과정 파라미터 : 최적화 퍼포먼스를 결정하는 파라미터
  - objective : 목적 함수. reg:linear, binary:logistic, count:poission 등 다양하다
  - eval_metric : 모델의 평가 함수를 조정하는 함수다. rmse, logloss, map 등 해당 테이터의 특성에 맞게 평가 함수를 조절한다.
- 커멘트 라인 파라미터
  - num_rounds : boosting 라운드를 결정한다. 랜덤하게 생성되는 모델이니 만큼 이 수가 적당히 큰 게 좋다. epoch 옵션과 동일

파라미터를 제대로 이해하고 있어야 모델을 구축하는데 시행착오가 적어진다 !!

파라미터의 우선순위

1. booster (부스터 모양)
2. eval_metric(평가 함수) / objective (목적 함수)
3. eta (러닝 레이트)
4. L1 form(L1 폼이 L2보다 민감하다)

파라미터 조정 우선순위는 사용자가 임의로 결정하는 부분이다.

train_test_split 함수

train / test를 분리하는 목적을 정확하게 알아야 한다. 정확히 말하면 train / test가 아닌 train / validation으로 볼 수 있다. 머신러닝 모델에 train 데이터fmf 100% 학습시킨 후 test 데이터에 모델을 적용했을 때 성능이 생각보다 안나오는 경우가 많다 <- 이러한 현상을 보통 오버피팅 되었다고 한다.

모델이 내가 가진 학습 데이터에 너무 과적합되도록 학습한 나머지, 이를 조금이라도 벗어난 케이스에 대해서는 예측율이 현지히 떨어지기 떄문이라고 이해하면 된다. 그렇기 때문에 오버피팅을 방지하는 것은 전체적인 모델 성능을 따져보았을 때 매우 중요한 프로세스 중 하나이다.

그래서 머신러닝 모델의 하이퍼 파라미터 튜닝 시 특히 n_estimators의 값은 validation 셋의 오차율을 점검해 나가면서 튜닝을 진행해야 하며, 딥러닝 모델도 마찬가지로 validation_data를 지정해 줌으로써 매 epoch마다 validation 오차율을 확인하면서 과적합을 방지해야 좋은 성능의 모델을 만들 수 있다.

그렇다면 epoch는 뭘까? 

batch gradient descent와 stochastic gradient descent의 합의 알고리즘인 mini-batch gradient desscent는 속도 및 일반화 측면에서 다른 최적화 기법에 비해 상대적으로 좋은 성적을 내므로 신경망과 같은 모델에서 자주 채택이 된다. 미니배치의 기본적인 아이디어는 전체 데이터 셋을 여러개의 미니 배치로 쪼개어 각각의 묶음에 대해 weight update값을 계산하여 local minima에 갇히는 상황을 회피하고자 SGD와 같이 빠른 속도로 최적점에 다가가는 것을 목표로 하는 알고리즘을 사용하는 것입니다. 이 후 해당 기법을 이용한 optimizer을 정의하고 훈련을 할 때 epoch, batch size 그리고 iteration 값을 조정해주게 된다. 

**Epoch**

사전적 정의 : 한 시대

기계학습에서 한 epoch란 훈련 데이터셋에 포함되어 있는 모든 데이터값들이 각각 한번씩 예측 모델에 들어온 뒤 weight값을 갱신하는 주기를 의미합니다.

예를  들어 손글씨를 판별하는 예측 모델을 훈련하기 위해 100장의 이미지와 그에 해당하는 label을 가지고 있다고 가정했을 때 첫번째부터 100번째까지 모든 이미지-데이터 쌍을 한번씩 모델에 넣어 훈련을 시켰다면 1epoch만큼의 훈련을 진행했다고 말할 수 있다. 기계학습 모델을 훈련시키기 위해 여러번의 epoch를 사용하는 이유는 이 후 새로운 데이터에 대한 일반화 능력을 향상시키고 각 epoch에서 local minima에 갇히는 상황을 보완하는데 도움을 주기 떄문입니다. 몇 번의 epoch를 사용하는 것이 가장 효율적인지는 데이터 셋에 따라 다르기 떄문에 직관적으로 결정하기 어렵다. 따라서 여러번의 실험을 거쳐 tuning을 해주어야 하는 hyper 파라미터의 일종이라고 설명할 수 있다.

**batch size**

말 그대로 하나의 mini-batch를 몇 개의 데이터로 구성할지에 대한 정보이다. 100개의 이미지 라벨이 있는 상황에서 batch size를 5로 정했다면, 우리의 모델은 한번에 하나의 데이터를 볼 때마다 매번 가중치 값을 갱신하지 않고 5개 데이터에 대한 값을 한 번에 계산한 뒤 가중치를 갱신해주게 되므로 20번의 갱신 과정만 거치게 된다. 

**Iteration**

한 epoch를 진행하기 위해 몇 번의 가중치 갱신이 이루어지는지에 대한 정보이다. 위의 예시에서는 가중치를 갱신하는 과정이 총 20번 이루어지므로 20이 iteration의 값이라고 볼 수 있다. batch size가 커질 수록 iteration 값은 줄어들고 역도 마찬가지이다. batch size또한 프로그래머가 직접 조정해주어야 하는 하이퍼 파라미터 일종으로 볼 수있으므로 실험을 통해 최적의 값을 찾는 과정이 필요하다.

**train / test 분리**

sklearn.model_selection에 있고 train_test_split을 사용하면 됨

test_size : 테스트 셋 구성의 비율을 나타낸다. 0.2 -> 20%를 test 셋으로 지정하겠다는 의미

**학습 종류에 따른 분류**

기계학습 문제들은 학습 종류에 따라 3가지로 나눌 수 있다.

- 지도학습
  - 사람이 교사로써 각각의 입력(x)와 레이블(y)을 달아놓은 데이터를 컴퓨터에 주면 컴퓨터가 그것을 학습하는 것이다. 사람이 직접 개입하므로 정확도가 높은 데이터를 사용할 수 있다는 장점이 있다. 대신에 사람이 직접 레이블을 달아야 하므로 인건비, 데이터 양이 적긴 하다.

- 분류

  - 레이블 y가 이산적인 경우, y가 가질 수 있는 값이 유한한 경우 분류, 혹은 인식 문제라고 부른다. 일상에서 접하기 쉬우며, 연구가 많이 되어있고, 기업들이 가장 관심을 가지는 문제중 하나이다. 이런 문제 해결을 위해 대표적인 기법으로는 로지스틱 회귀법, KNN, SVM, Decision tree 등이 있다.

  ex) 주차게이트에서 번호판 인식 / 페이스북이나 구글 포토의 얼굴 인식

  

- 회귀

  - 레이블 y가 실수인 경우 회귀문제라고 한다. 보통 엑셀에서 그래프 그릴 때 많이 접근하는 것이다. 데이터를 쭉 뿌려놓고 이것을 가장 잘 설명하는 직선 하나 혹은 이차함수 곡선 하나를 그리고 싶을 때 회귀 기능을 사용한다. 잘 생각해보면 데이터는 입력(x)와 실수 레이블(y)의 짝으로 이루어져 있고, 새로운 임의의 입력(x)에 대해 y를 맞추는 것이 바로 직선 혹은 곡선이므로 기계학습 문제가 맞다. 통계학의 회귀분석 기법 중 선형회귀 기법이 이에 해당하는 대표적인 예이다.

- 







